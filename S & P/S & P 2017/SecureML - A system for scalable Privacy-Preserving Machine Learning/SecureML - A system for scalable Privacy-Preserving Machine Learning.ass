[Script Info]
; Script generated by Aegisub 3.2.2
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: TV.601
PlayResX: 1280
PlayResY: 720

[Aegisub Project Garbage]
Audio File: CCS 2016 - On the Security and Performance of Proof of Work Blockchains.mp4
Video File: CCS 2016 - On the Security and Performance of Proof of Work Blockchains.mp4
Video AR Mode: 4
Video AR Value: 1.777778
Video Zoom Percent: 0.500000
Scroll Position: 78
Active Line: 82
Video Position: 33209

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,微软雅黑,45,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.00,0:00:05.90,Default,,0,0,0,,{\pos(639,50)}听译、时间轴：刘巍然（学酥）
Dialogue: 0,0:00:09.34,0:00:10.20,Default,,0,0,0,,大家好
Dialogue: 0,0:00:10.20,0:00:13.76,Default,,0,0,0,,我是Yupeng Zhang 来自马里兰大学
Dialogue: 0,0:00:13.76,0:00:16.26,Default,,0,0,0,,今天 我要讲解我们撰写的论文
Dialogue: 0,0:00:16.26,0:00:20.54,Default,,0,0,0,,安全机器学习：一个可扩展隐私保护机器学习系统
Dialogue: 0,0:00:21.28,0:00:24.80,Default,,0,0,0,,这个工作由我和来自VISA研究院的Payman共同完成
Dialogue: 0,0:00:27.48,0:00:33.34,Default,,0,0,0,,现今 机器学习已被用在各个领域中 且引发了各个领域的变革
Dialogue: 0,0:00:33.34,0:00:43.90,Default,,0,0,0,,例如 机器学习可以用于图像处理、语音识别、异常检测、甚至下围棋
Dialogue: 0,0:00:44.84,0:00:47.72,Default,,0,0,0,,机器学习之所以在实际中的应用效果如此之好
Dialogue: 0,0:00:47.72,0:00:54.36,Default,,0,0,0,,是因为我们使用了大量的数据来训练机器学习模型
Dialogue: 0,0:00:54.36,0:01:00.36,Default,,0,0,0,,虽然机器学习引发了变革 此技术也引入了安全问题
Dialogue: 0,0:01:00.36,0:01:05.00,Default,,0,0,0,,我们应该如何保护数据的隐私性？
Dialogue: 0,0:01:05.00,0:01:09.70,Default,,0,0,0,,这里需要澄清一点 这里所指的隐私性与前面讲座中的隐私性不太相同
Dialogue: 0,0:01:09.70,0:01:15.56,Default,,0,0,0,,这里我们考虑的是用于训练机器学习模型的数据 如何保证这些数据的隐私性
Dialogue: 0,0:01:15.56,0:01:24.96,Default,,0,0,0,,毕竟作为终端用户 我们不想向公司分享我们的数据 让它们可以运行机器学习算法
Dialogue: 0,0:01:24.96,0:01:26.80,Default,,0,0,0,,我们应该如何解决这个问题？
Dialogue: 0,0:01:28.38,0:01:33.58,Default,,0,0,0,,隐私保护机器学习为这类安全问题提供了一个解决方案
Dialogue: 0,0:01:33.58,0:01:43.16,Default,,0,0,0,,它允许公司执行相同的机器学习算法 但不需要得知用户的实际数据
Dialogue: 0,0:01:43.16,0:01:46.92,Default,,0,0,0,,这样一来 用户仍然可以获得机器学习算法带来的益处
Dialogue: 0,0:01:46.92,0:01:52.18,Default,,0,0,0,,同时可以实现数据隐私保护、不将数据泄露给公司
Dialogue: 0,0:01:53.46,0:01:58.64,Default,,0,0,0,,隐私保护机器学习研究领域已经有了很多的前置学术成果
Dialogue: 0,0:01:58.64,0:02:00.90,Default,,0,0,0,,幻灯片上列举了其中一些论文
Dialogue: 0,0:02:00.90,0:02:03.92,Default,,0,0,0,,这是一个非常前沿 进展速度很快的研究领域
Dialogue: 0,0:02:05.84,0:02:12.34,Default,,0,0,0,,在我们的论文中 我们聚焦于下述安全模型：双服务器模型
Dialogue: 0,0:02:13.94,0:02:19.36,Default,,0,0,0,,在这个模型中 我们假定两个服务器分别属于两个不同的公司
Dialogue: 0,0:02:19.36,0:02:22.42,Default,,0,0,0,,两个服务器不会实施共谋攻击
Dialogue: 0,0:02:23.82,0:02:31.74,Default,,0,0,0,,作为终端用户 我们首先将数据拆分成两个分享值 分别将分享值发送给两个服务器
Dialogue: 0,0:02:31.74,0:02:40.56,Default,,0,0,0,,这样 单一服务器无法得到原始数据任何信息 因为它只能得到其中一个分享值
Dialogue: 0,0:02:42.70,0:02:51.88,Default,,0,0,0,,随后 两个服务器互相交互 执行两方安全计算 生成机器学习模型
Dialogue: 0,0:02:51.88,0:02:54.96,Default,,0,0,0,,这一安全模型的优点在于
Dialogue: 0,0:02:54.96,0:03:03.34,Default,,0,0,0,,首先 此模型将多方安全计算过程归约为两方安全计算过程 以此大幅提高计算效率
Dialogue: 0,0:03:03.34,0:03:10.94,Default,,0,0,0,,其次 上传数据后 用户即可处于离线状态 模型训练过程中用户不需要与服务器交互
Dialogue: 0,0:03:10.94,0:03:20.46,Default,,0,0,0,,此模型也可以解决下述问题：两个公司想共同训练模型 但不想将数据分享给对方
Dialogue: 0,0:03:20.46,0:03:23.50,Default,,0,0,0,,很多前置学术成果都使用了这一安全模型
Dialogue: 0,0:03:25.96,0:03:28.98,Default,,0,0,0,,本篇论文的主要贡献是
Dialogue: 0,0:03:28.98,0:03:37.16,Default,,0,0,0,,在此安全模型下 我们提出新的协议 支持隐私保护线性回归、逻辑回归、神经网络
Dialogue: 0,0:03:38.12,0:03:47.34,Default,,0,0,0,,特别地 我们综合使用了秘密分享、预计算三元组代数运算、以及混淆电路技术
Dialogue: 0,0:03:49.20,0:03:55.06,Default,,0,0,0,,从实现角度看 我们的系统与前置工作相比 效率有了量级上的提升
Dialogue: 0,0:03:55.06,0:04:00.00,Default,,0,0,0,,我们的系统支持大数据集模型训练 可以支持百万量级的数据集 五千个特征
Dialogue: 0,0:04:01.00,0:04:05.46,Default,,0,0,0,,本次讲座 我们主要关注线性回归和逻辑回归
Dialogue: 0,0:04:05.46,0:04:09.06,Default,,0,0,0,,相关技术可以推广到神经网络模型训练中
Dialogue: 0,0:04:09.06,0:04:12.12,Default,,0,0,0,,大家可以阅读我们的论文 以了解更多的技术细节
Dialogue: 0,0:04:14.66,0:04:16.98,Default,,0,0,0,,首先 什么是线性回归？
Dialogue: 0,0:04:18.36,0:04:23.50,Default,,0,0,0,,假设我们将数据点和与之相关的结果值画在图中 如幻灯片所示
Dialogue: 0,0:04:23.50,0:04:30.38,Default,,0,0,0,,线性回归要根据图中的点 尝试拟合出一条与点尽可能吻合的线
Dialogue: 0,0:04:30.38,0:04:35.20,Default,,0,0,0,,形式化地讲 输入是数据值对x和y
Dialogue: 0,0:04:35.20,0:04:38.46,Default,,0,0,0,,这里x可以是一个向量 我们也称x为特征值
Dialogue: 0,0:04:38.46,0:04:43.50,Default,,0,0,0,,y是一个单值 我们也称y为标签值
Dialogue: 0,0:04:45.04,0:04:52.54,Default,,0,0,0,,输出的模型w是一个系数向量 其维度大小与输入x的维度大小相同
Dialogue: 0,0:04:52.54,0:04:59.94,Default,,0,0,0,,我们要求w和x的内积结果应该与y值非常近似
Dialogue: 0,0:04:59.94,0:05:02.70,Default,,0,0,0,,实际上 模型定义了x和y之间的线性关系
Dialogue: 0,0:05:04.06,0:05:10.34,Default,,0,0,0,,为了训练模型 我们这里引入随机梯度下降算法 简称SGD算法
Dialogue: 0,0:05:10.34,0:05:13.54,Default,,0,0,0,,我们把这个问题看成一个最优化问题
Dialogue: 0,0:05:13.54,0:05:19.04,Default,,0,0,0,,尝试找到一个最佳模型w 使得y*和y的距离取得最小值
Dialogue: 0,0:05:20.54,0:05:24.70,Default,,0,0,0,,算法首先将w初始化到一个随机位置上
Dialogue: 0,0:05:24.70,0:05:27.58,Default,,0,0,0,,随后 算法从数据集中选取一个随机样本
Dialogue: 0,0:05:28.96,0:05:35.20,Default,,0,0,0,,算法根据当前模型w计算预测值 并将结果与正确的标签值进行比较
Dialogue: 0,0:05:35.20,0:05:39.72,Default,,0,0,0,,比较结果将告诉算法 应该往哪个方向移动 以得到最优解
Dialogue: 0,0:05:39.72,0:05:46.86,Default,,0,0,0,,可以证明 如果重复上述步骤 最终模型会收敛到最优位置
Dialogue: 0,0:05:46.86,0:05:49.02,Default,,0,0,0,,这就是SGD算法
Dialogue: 0,0:05:49.02,0:05:55.68,Default,,0,0,0,,对于线性回归这一特殊算法 更新函数可以用幻灯片上的公式表示
Dialogue: 0,0:05:55.68,0:05:58.82,Default,,0,0,0,,正如大家所看到的 更新公式非常简单
Dialogue: 0,0:05:58.82,0:06:02.08,Default,,0,0,0,,更新公式只涉及到乘法运算和减法运算
Dialogue: 0,0:06:03.06,0:06:07.00,Default,,0,0,0,,因此 一种很自然地实现隐私保护线性回归的方法是
Dialogue: 0,0:06:07.00,0:06:12.46,Default,,0,0,0,,将秘密分享与分享值代数运算直接应用到线性回归算法上
Dialogue: 0,0:06:12.46,0:06:14.44,Default,,0,0,0,,这应该可以解决隐私问题
Dialogue: 0,0:06:14.44,0:06:16.46,Default,,0,0,0,,整个场景描述如下
Dialogue: 0,0:06:16.46,0:06:19.82,Default,,0,0,0,,用户首先将数据和标签值进行秘密分享
Dialogue: 0,0:06:19.82,0:06:25.04,Default,,0,0,0,,服务器随机初始化模型 同样对模型执行秘密分享
Dialogue: 0,0:06:25.04,0:06:34.98,Default,,0,0,0,,随后 我们直接应用预计算的三元组 一遍又一遍地在分享值上执行更新函数
Dialogue: 0,0:06:35.54,0:06:38.66,Default,,0,0,0,,这就能解决问题了
Dialogue: 0,0:06:38.66,0:06:42.26,Default,,0,0,0,,但这里有一个很大的问题
Dialogue: 0,0:06:42.26,0:06:51.04,Default,,0,0,0,,因为秘密分享和分享值代数运算只能在整数域上执行 例如在模质数下的整数域执行
Dialogue: 0,0:06:51.04,0:06:58.78,Default,,0,0,0,,但只有当参数带小数时 线性回归和SGD算法才能正确执行
Dialogue: 0,0:06:58.78,0:07:02.32,Default,,0,0,0,,我们如何在整数域上实现带小数的运算？
Dialogue: 0,0:07:04.18,0:07:06.58,Default,,0,0,0,,这就引出了我们的第一个贡献
Dialogue: 0,0:07:06.58,0:07:11.20,Default,,0,0,0,,我们给出了一种方法 可以在整数域上直接执行带小数乘法
Dialogue: 0,0:07:11.20,0:07:13.56,Default,,0,0,0,,具体思想如下
Dialogue: 0,0:07:13.56,0:07:18.68,Default,,0,0,0,,考虑存在两个幻灯片上所示的带小数点的数 我们知道这两个数对应的明文
Dialogue: 0,0:07:18.68,0:07:23.56,Default,,0,0,0,,随后 在不丧失计算精度的条件下将两个数相乘
Dialogue: 0,0:07:23.56,0:07:27.72,Default,,0,0,0,,我们可以得到结果c 其中c的小数部分为原来的2倍
Dialogue: 0,0:07:27.72,0:07:31.56,Default,,0,0,0,,这里我们假定整数部分足够大 不会超过有限域的范围
Dialogue: 0,0:07:32.82,0:07:39.48,Default,,0,0,0,,如果不考虑小数点 小数部分的乘法运算和整数部分的乘法运算完全一致 这很不错
Dialogue: 0,0:07:39.48,0:07:42.00,Default,,0,0,0,,问题在于c会变长
Dialogue: 0,0:07:42.00,0:07:46.48,Default,,0,0,0,,如此计算下去 c的长度会越来越长 最终超过整数域的范围 最终导致溢出
Dialogue: 0,0:07:46.48,0:07:54.26,Default,,0,0,0,,解决此问题的一种直观方法是进行截断 即直接扔掉c的小数点最后几位
Dialogue: 0,0:07:54.26,0:07:56.82,Default,,0,0,0,,这样一来 c的长度就和a、b相同了
Dialogue: 0,0:07:56.82,0:08:01.10,Default,,0,0,0,,这一方法称为定点乘法
Dialogue: 0,0:08:01.10,0:08:07.84,Default,,0,0,0,,在论文中我们证明：可以在分享值上应用相同的截断技巧
Dialogue: 0,0:08:07.84,0:08:13.50,Default,,0,0,0,,具体来说 这里我们有两个服务器上分享的a和b
Dialogue: 0,0:08:13.50,0:08:17.76,Default,,0,0,0,,也就是说 这两个数分别被一个大整数域上的大随机数所遮盖
Dialogue: 0,0:08:19.60,0:08:25.94,Default,,0,0,0,,随后 我们应用预计算三元组执行乘法操作 得到c的分享值 即c_0和c_1
Dialogue: 0,0:08:25.94,0:08:30.78,Default,,0,0,0,,c_0和c_1中编码了全精度乘法计算结果
Dialogue: 0,0:08:32.14,0:08:37.10,Default,,0,0,0,,随后 两个服务器分别独立地对c_0和c_1进行截断 此过程不引入任何通信开销
Dialogue: 0,0:08:37.10,0:08:45.68,Default,,0,0,0,,我们证明截断后 应用两个分享值仍然可以以很高的概率恢复出定点乘法的计算结果
Dialogue: 0,0:08:45.68,0:08:51.16,Default,,0,0,0,,只不过计算结果的最后一位小数上会增加一个非常小的误差值
Dialogue: 0,0:08:51.16,0:08:52.86,Default,,0,0,0,,这就是我们的技术方案
Dialogue: 0,0:08:54.12,0:09:01.18,Default,,0,0,0,,应用此技术方案 回到协议层面上 每一次乘法运算中 我们都对结果分享值简单截断
Dialogue: 0,0:09:01.18,0:09:06.78,Default,,0,0,0,,这样就完成了整个隐私保护线性回归协议的实现
Dialogue: 0,0:09:08.18,0:09:10.50,Default,,0,0,0,,这里向大家展示截断技术的应用效果
Dialogue: 0,0:09:10.50,0:09:14.10,Default,,0,0,0,,由于此技术只在计算结果的最后一个比特中引入了非常小的噪声
Dialogue: 0,0:09:14.10,0:09:21.22,Default,,0,0,0,,因此 整个计算过程的执行时间几乎和在明文上应用小数执行整个计算过程的时间相同
Dialogue: 0,0:09:21.22,0:09:29.10,Default,,0,0,0,,具体来说 在各种不同的场景下 我们所提出的技术要比定点乘法混淆电路快4-8倍
Dialogue: 0,0:09:31.06,0:09:32.62,Default,,0,0,0,,线性回归部分就这些内容
Dialogue: 0,0:09:34.12,0:09:36.78,Default,,0,0,0,,下一部分 逻辑回归
Dialogue: 0,0:09:36.78,0:09:40.26,Default,,0,0,0,,逻辑回归主要用在分类问题上
Dialogue: 0,0:09:40.26,0:09:44.04,Default,,0,0,0,,我们要尝试将数据分成两个类型
Dialogue: 0,0:09:46.06,0:09:50.28,Default,,0,0,0,,形式化地讲 逻辑回归中的数据值对和线性回归相同
Dialogue: 0,0:09:50.28,0:09:54.86,Default,,0,0,0,,但在逻辑回归中 y是一个比特值 取值为0或者1 分别表示两种分类结果
Dialogue: 0,0:09:56.30,0:10:05.22,Default,,0,0,0,,逻辑回归和线性回归的区别是 我们要在内积结果上进一步执行一个额外的函数f
Dialogue: 0,0:10:05.22,0:10:08.08,Default,,0,0,0,,此函数f一般称为激活函数
Dialogue: 0,0:10:09.48,0:10:15.14,Default,,0,0,0,,逻辑回归中的激活函数f为1/(1+e^(-u))
Dialogue: 0,0:10:15.14,0:10:17.80,Default,,0,0,0,,函数图像如幻灯片所示
Dialogue: 0,0:10:19.56,0:10:22.88,Default,,0,0,0,,我们仍然可以使用SGD算法来训练模型
Dialogue: 0,0:10:22.88,0:10:27.18,Default,,0,0,0,,令人惊讶的是 逻辑回归的更新函数几乎与线性回归完全相同
Dialogue: 0,0:10:27.18,0:10:30.98,Default,,0,0,0,,唯一的区别是我们要在内积结果上额外调用一次激活函数f
Dialogue: 0,0:10:30.98,0:10:34.52,Default,,0,0,0,,更新函数的其它地方都与线性回归完全相同
Dialogue: 0,0:10:34.52,0:10:40.36,Default,,0,0,0,,这意味着如果我们可以通过安全多方计算的方式计算函数f
Dialogue: 0,0:10:40.36,0:10:44.34,Default,,0,0,0,,我们就可以把这个计算过程应用到原始线性回归协议中 即可实现逻辑回归
Dialogue: 0,0:10:45.48,0:10:47.36,Default,,0,0,0,,但事实证明 这会面临巨大的挑战
Dialogue: 0,0:10:47.36,0:10:53.10,Default,,0,0,0,,因为函数f涉及到精确到小数的自然对数求幂
Dialogue: 0,0:10:53.10,0:10:54.86,Default,,0,0,0,,如何实现此计算过程？
Dialogue: 0,0:10:56.04,0:11:00.10,Default,,0,0,0,,传统方法是应用所谓的多项式近似方式
Dialogue: 0,0:11:00.10,0:11:04.12,Default,,0,0,0,,幻灯片给出了10阶多项式近似激活函数f的图像
Dialogue: 0,0:11:04.12,0:11:06.96,Default,,0,0,0,,正如大家所看到的 近似图像与逻辑回归激活函数非常接近
Dialogue: 0,0:11:06.96,0:11:12.66,Default,,0,0,0,,但是通过安全计算方式实现近似函数的计算 会引入较大的计算开销
Dialogue: 0,0:11:12.66,0:11:17.76,Default,,0,0,0,,因为我们至少需要执行10次乘法计算 才能完成10阶近似多项式的计算过程
Dialogue: 0,0:11:19.00,0:11:24.88,Default,,0,0,0,,在我们的论文中 我们重点考虑 激活函数的作用究竟是什么
Dialogue: 0,0:11:24.88,0:11:30.92,Default,,0,0,0,,因为我们要解决的是分类问题 我们实际需要的是一个值域为[0,1]的激活函数
Dialogue: 0,0:11:30.92,0:11:33.20,Default,,0,0,0,,并且此函数在0点附近应该大幅递增
Dialogue: 0,0:11:33.20,0:11:36.50,Default,,0,0,0,,那么 我们能不能用这样一个函数作为激活函数？
Dialogue: 0,0:11:36.50,0:11:43.36,Default,,0,0,0,,我们证明 把此函数作为激活函数 所得模型的准确性和原始逻辑回归函数准确性相同
Dialogue: 0,0:11:43.36,0:11:50.12,Default,,0,0,0,,但更重要的是 我们可以应用混淆电路高效地通过安全多方计算方式实现此激活函数
Dialogue: 0,0:11:50.12,0:11:55.22,Default,,0,0,0,,此激活函数只涉及到减法运算和与0比大小 后者本质上是查看最高位比特值是否为0
Dialogue: 0,0:11:57.02,0:12:01.28,Default,,0,0,0,,因此 这引出了我们论文的另一个贡献
Dialogue: 0,0:12:01.28,0:12:06.00,Default,,0,0,0,,我们提出了一个新的概念：适用于安全多方计算的激活函数
Dialogue: 0,0:12:06.00,0:12:11.22,Default,,0,0,0,,我们不再通过已有方法近似计算激活函数
Dialogue: 0,0:12:11.22,0:12:16.68,Default,,0,0,0,,我们后退一步 思考我们到底需要满足何种条件的激活函数
Dialogue: 0,0:12:16.68,0:12:21.94,Default,,0,0,0,,随后 我们尝试提出一个新的激活函数 其可以高效地通过多方安全计算的方式实现
Dialogue: 0,0:12:24.56,0:12:29.72,Default,,0,0,0,,回到协议中来 我前面也讲到 我们只需要执行和线性回归相同的协议
Dialogue: 0,0:12:29.72,0:12:37.14,Default,,0,0,0,,在计算内积结果后 我们转换到混淆电路上计算激活函数的结果 再切换回原始协议中
Dialogue: 0,0:12:37.14,0:12:40.62,Default,,0,0,0,,这就是隐私保护逻辑回归的完整协议了
Dialogue: 0,0:12:44.34,0:12:47.70,Default,,0,0,0,,我们还在论文中引入了一些其它的优化方法
Dialogue: 0,0:12:47.70,0:12:52.14,Default,,0,0,0,,如向量化 即所有计算过程都可以用矩阵形式表示
Dialogue: 0,0:12:52.14,0:12:56.14,Default,,0,0,0,,这种方式可以大幅提高计算效率
Dialogue: 0,0:12:56.14,0:12:59.02,Default,,0,0,0,,进一步 这一技术可以推广到神经网络训练中
Dialogue: 0,0:13:01.26,0:13:03.82,Default,,0,0,0,,最后 我给大家讲解我们的实验结果
Dialogue: 0,0:13:05.30,0:13:12.72,Default,,0,0,0,,我们在包含10万条数据、每条数据包含500个特征的数据集上进行了实验
Dialogue: 0,0:13:14.84,0:13:18.82,Default,,0,0,0,,我们的协议可以很自然地分为2个阶段
Dialogue: 0,0:13:18.82,0:13:24.60,Default,,0,0,0,,第一个阶段是与数据无关的离线阶段 此阶段要生成乘法三元组
Dialogue: 0,0:13:24.60,0:13:27.42,Default,,0,0,0,,第二个阶段是在线阶段 此阶段要训练算法
Dialogue: 0,0:13:28.74,0:13:34.90,Default,,0,0,0,,在局域网环境下 网络带宽为1.2GB/s 网络时延为0.17ms
Dialogue: 0,0:13:37.70,0:13:42.78,Default,,0,0,0,,协议的离线阶段需要花费大约400秒
Dialogue: 0,0:13:42.78,0:13:45.64,Default,,0,0,0,,协议的在线阶段执行速度非常快
Dialogue: 0,0:13:45.64,0:13:51.96,Default,,0,0,0,,只需要花费1.4秒 只比明文数据训练慢2倍
Dialogue: 0,0:13:54.10,0:14:00.40,Default,,0,0,0,,在广域网环境下 网络带宽为9MB/s 网络时延为72ms
Dialogue: 0,0:14:00.40,0:14:07.58,Default,,0,0,0,,离线阶段大约要花费9000秒 在线阶段要花费141秒
Dialogue: 0,0:14:09.04,0:14:16.14,Default,,0,0,0,,即使在广域网环境下 我们系统的执行效率也要比前置工作快54倍
Dialogue: 0,0:14:18.06,0:14:22.68,Default,,0,0,0,,进一步 我们观察到离线阶段是我们系统的性能瓶颈
Dialogue: 0,0:14:22.68,0:14:28.58,Default,,0,0,0,,我们进一步提出了一个替代方案 在用户的帮助下生成乘法三元组
Dialogue: 0,0:14:28.58,0:14:36.68,Default,,0,0,0,,应用这一方案 我们可以大幅降低离线阶段的时间开销 稍稍提高在线阶段的时间开销
Dialogue: 0,0:14:36.68,0:14:40.34,Default,,0,0,0,,不过此方案会减弱系统的安全模型
Dialogue: 0,0:14:40.34,0:14:46.64,Default,,0,0,0,,如果应用此方案 我们需要进一步假设客户端不与任意一个服务器实施共谋攻击
Dialogue: 0,0:14:48.68,0:14:52.56,Default,,0,0,0,,接下来是逻辑回归的实验结果
Dialogue: 0,0:14:52.56,0:14:56.62,Default,,0,0,0,,正如我前面所讲到的那样 我们协议的一大优势在于
Dialogue: 0,0:14:56.62,0:15:04.36,Default,,0,0,0,,与线性回归相比 逻辑回归不会额外使用更多的预计算乘法三元组
Dialogue: 0,0:15:04.36,0:15:07.80,Default,,0,0,0,,因此 逻辑回归离线阶段时间消耗与线性回归离线阶段的时间消耗相同
Dialogue: 0,0:15:09.64,0:15:15.76,Default,,0,0,0,,在线阶段 我们需要进一步执行一次混淆电路 并引入一次额外的信息交互
Dialogue: 0,0:15:15.76,0:15:18.62,Default,,0,0,0,,额外增加的时间消耗如幻灯片所示
Dialogue: 0,0:15:18.62,0:15:21.32,Default,,0,0,0,,总消耗时间与线性回归仍处在同一个量级
Dialogue: 0,0:15:22.74,0:15:32.12,Default,,0,0,0,,据我们所知 这是在此安全模型下第一个实现隐私保护逻辑回归的研究成果
Dialogue: 0,0:15:32.12,0:15:36.90,Default,,0,0,0,,此系统可以支持100万条数据集 每条数据集包含5000个特征
Dialogue: 0,0:15:38.62,0:15:40.72,Default,,0,0,0,,最后是神经网络
Dialogue: 0,0:15:40.72,0:15:46.00,Default,,0,0,0,,我们实现了包含2个隐藏层 每层包含128个神经元的神经网络
Dialogue: 0,0:15:46.00,0:15:51.12,Default,,0,0,0,,这里我们直接给出端到端的性能测试结果 综合考虑了在线阶段和离线阶段
Dialogue: 0,0:15:51.12,0:15:56.42,Default,,0,0,0,,在局域网环境下 训练此神经网络的时间大约为25,000秒
Dialogue: 0,0:15:56.42,0:16:01.44,Default,,0,0,0,,训练所消耗的时间是明文训练所消耗时间的35倍
Dialogue: 0,0:16:03.12,0:16:05.84,Default,,0,0,0,,在广域网环境下 性能会变得更加糟糕
Dialogue: 0,0:16:05.84,0:16:12.26,Default,,0,0,0,,对于此量级的数据集 总训练时间约为200,000秒
Dialogue: 0,0:16:15.20,0:16:16.90,Default,,0,0,0,,最后是总结
Dialogue: 0,0:16:16.90,0:16:24.60,Default,,0,0,0,,本文提出了一个新的协议 实现隐私保护线性回归、逻辑回归、神经网络
Dialogue: 0,0:16:24.60,0:16:30.56,Default,,0,0,0,,特别地 我们引入了一种新的方法 可以直接在整数域下实现带小数乘法
Dialogue: 0,0:16:31.88,0:16:36.58,Default,,0,0,0,,我们提出了一个容易通过安全多方计算方式实现的激活函数
Dialogue: 0,0:16:36.58,0:16:38.48,Default,,0,0,0,,我们引入了向量化优化方式
Dialogue: 0,0:16:39.66,0:16:46.70,Default,,0,0,0,,从实现角度看 我们系统的执行效率比前置工作高几个数量级 可以支持大数据集训练
Dialogue: 0,0:16:47.92,0:16:50.08,Default,,0,0,0,,这就是我讲座的全部内容了 谢谢大家
Dialogue: 0,0:16:56.14,0:16:59.86,Default,,0,0,0,,台下有两个麦克风 听众可以用麦克风提问
Dialogue: 0,0:17:06.24,0:17:08.24,Default,,0,0,0,,我来问个问题
Dialogue: 0,0:17:08.24,0:17:18.78,Default,,0,0,0,,在SGD算法下 你们给出了三个技术提高了安全多方计算场景下SGD算法的效率
Dialogue: 0,0:17:18.78,0:17:27.12,Default,,0,0,0,,你是否可以介绍一下 每个技术分别对算法提供了多大的优化量？
Dialogue: 0,0:17:27.12,0:17:30.32,Default,,0,0,0,,我们在论文中给出了详细的基准测试结果
Dialogue: 0,0:17:30.32,0:17:41.54,Default,,0,0,0,,简单总结一下 与通用方案相比 每个独立的技术都将算法速度提高了10倍左右
Dialogue: 0,0:17:41.54,0:17:44.04,Default,,0,0,0,,但需要把这几个技术组合起来使用
Dialogue: 0,0:17:44.04,0:17:47.84,Default,,0,0,0,,例如 带小数乘法运算不能用在混淆电路上
Dialogue: 0,0:17:47.84,0:17:52.68,Default,,0,0,0,,你只能在分享值代数计算过程中应用带小数乘法计算的相关优化技术
Dialogue: 0,0:17:52.68,0:17:55.82,Default,,0,0,0,,把各技术综合起来 算法的总执行效率会提高好几个量级
Dialogue: 0,0:17:56.56,0:17:57.50,Default,,0,0,0,,非常感谢
Dialogue: 0,0:17:58.68,0:18:01.44,Default,,0,0,0,,你好 我是John Percival 来自罗切斯特大学
Dialogue: 0,0:18:01.44,0:18:03.16,Default,,0,0,0,,我有个很简单的问题
Dialogue: 0,0:18:03.86,0:18:12.98,Default,,0,0,0,,你们是通过仿真完成的广域网实验 还是在真实的广域网环境下完成的实验？
Dialogue: 0,0:18:12.98,0:18:15.28,Default,,0,0,0,,我们在亚马逊的机器上实现了我们的方案
Dialogue: 0,0:18:15.28,0:18:19.84,Default,,0,0,0,,一台机器位于美国东海岸 一台机器位于美国西海岸
Dialogue: 0,0:18:19.84,0:18:23.02,Default,,0,0,0,,因此这是个真实的实验 没有仿真过程
Dialogue: 0,0:18:23.02,0:18:24.54,Default,,0,0,0,,好的 谢谢
Dialogue: 0,0:18:28.06,0:18:30.58,Default,,0,0,0,,好的 我们再次对演讲者表示感谢
Dialogue: 0,0:18:30.44,0:18:35.44,Default,,0,0,0,,{\pos(639,50)}听译、时间轴：刘巍然（学酥）
